\section{Regression}

\subsection*{Bias-Variance trade-off}
Let $D$ be the training dataset and $\hat{f}$ be the predictive function.
$ \E_D \E_{Y\mid X} (\hat{f}(X)-Y)^2 = \E_D \E_{Y\mid X} [(\hat{f}(X)-\E_{Y\mid X}Y)^2 + (\E_{Y\mid X}Y-Y)^2] = \E_D (\hat{f}(X)-\E (Y\mid X))^2 + \E_D (\E(Y\mid X)-Y)^2 = \E_D (\hat{f}(x)-\E_D \hat{f}(x))^2 + \left(\E_D \hat{f}(x)-\E(Y\mid X)\right)^2 + \E_D (\E(Y\mid X)-Y)^2$. It means that expected square error (training) = variance of prediction + squared bias + variance of noise.

The optimal trade-off is achieved by avoiding under-fitting (large bias) and over-fitting (large variance). Note that here the variance of output is computed by refitting the regressor on a new dataset.

\subsection*{Regularization}

Ridge and Lasso can be viewed as MAP (maximum a posterior) estimation. A Gaussian prior on $\beta$ is equivalent to Ridge and a Laplacian prior is equivalent to Lasso. Using SVD, we get Ridge has built-in model selection:  $X\beta^{\text{Ridge}} = \sum_{j=1}^d [d_j^2 /(d_j^2+\lambda)] u_j u_j^T Y$ (each $u_j u_j^T Y$ can be viewed as a model). Lasso has more sparse estimations because the gradient of regularization does not shrink as in the case of Ridge.