\section{Ensemble}

\subsection*{Random Forest}

Random feature selection induces regulation.

\subsection*{Adaboost}

Classifier weight $\alpha_t=\frac{1}{\text{weighted err}_t}-1$. Sample weight $w_{t+1}=\alpha_t w_t$ for mislabeled samples, o.w. unchanged.

Adaboost has following properties:
\begin{enumerate}
    \item It minimizes exponential loss forwardly.
    \item It trains max-margin classifiers.
    \item It, as well as Random Forest, is spiky self-averaging interpolators, which localize the effect of noise.
    \item It falls into the double descent regime: over-parameterized models can have better generalization.
\end{enumerate}