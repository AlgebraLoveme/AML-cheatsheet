\section{BLR and GP}

\subsection*{Bayesian Linear Regression}
\begin{scriptsize}$Y=X\beta+\epsilon$\textasciitilde$\normal(0, \sigma^2)$. Prior $\beta $\textasciitilde$ \normal(0, \Lambda^{-1})$. Posterior $\beta | X, Y $\\\textasciitilde$ \normal(\mu_\beta, \Sigma_\beta)$, $\Sigma_\beta =  (\sigma^{-2}X^T X+\Lambda)^{-1}$, $\mu_\beta = \sigma^2\Sigma_{\beta}X^T Y$. \end{scriptsize}

% MAP estimation of this prior (i.e., $\mu_\beta$) is equivalent to Ridge regression given $\Lambda = \lambda I$ and $\sigma=1$.

% When $\Lambda=\lambda I$, under the prior, $Y$\textasciitilde$ \normal(0, \frac{1}{\lambda}X^T X+\sigma^2 I)$. Therefore, $\cov(y_i, y_j) = \frac{1}{\lambda}x_i^T x_j$. It means a prior that closer samples is more similar, i.e., $\cov(y_i, y_j)$ is large when $x_i^T x_j$ is large. The kernel $X^T \Lambda^{-1} X$ is thus called linear kernel. When a general kernel is used, Gaussian Process appears.

\subsection*{Gaussian Process}
$Y = \left(\begin{matrix} Y_0 \\ Y_1 \end{matrix}\right)$ is the combination of observed and prediction value. Assume a Gaussian prior of $\mathcal{N}(0, K + \sigma^2 I)$, where $K_{ij}=k(x_i,x_j)$ is kernel. GP regression is the conditional/Posterior distribution on $Y_0$, $\mathbb{E} [Y_1|Y_0] = K_{1 0}(\sigma^{2} I_{0}+K_{00})^{-1}Y_0$, $\mathrm{Cov}[Y_1] = \sigma^{2}I_{1} + K_{11} - K_{10} (\sigma^2  I_{0} + K_{00})^{-1} K_{01}$. Bayesian LR is a special case of GP with linear kernel $k(x,y) = x^{\top}\Lambda^{-1}y$.

\subsection*{Kernel Function}
A function is a kernel iff (1) symmetry $k(x, x^\prime)=k(x^\prime, x)$ and (2) semi-positive definite $\int_\Omega k(x, x^\prime) f(x) f(x^\prime) dx dx^\prime \ge 0$ for any $f \in L_2$ and $\Omega \in \mathcal{R}^d$ (continuous) or $K(X)\succeq 0$ (discrete). The latter is equivalent to (1) $a^{\top} K a \geq 0, \forall a$ or (2) $k(x, x^\prime) = \phi(x)^T \phi(x^\prime)$ for some $\phi$.

\subsection*{Kernel Construction}
If $k_{1,2}$ are valid kernels, then followings are valid: (1) $k(x, x^\prime) = k_1(x, x^\prime) + k_2(x, x^\prime)$. (2) $k(x, x^\prime) = k_1(x, x^\prime)\cdot k_2(x, x^\prime)$. Proof: expand by Mercer's thm. (3) $k(x, x^\prime) = c k_1(x, x^\prime)$ for constant $c>0$. (4) $k(x, x^\prime) = f(k_1(x, x^\prime))$ if $f$ is a polynomial with positive coefficients or the exp. Proof: polynomial can be proved by applying the product, positive scaling and addition. Exp can be proved by taking limit on the polynomial. (5) $k(x, x^\prime) = f(x) k_1(x, x^\prime) f(x^\prime)$. (6) $k(x, x^\prime) = k_1(\phi(x), \phi(x^\prime))$ for any function $\phi$.


Example: RBF kernel $k(x,y) = e^{-||x-y||^2/2\sigma^2} = e^{-||x||^2/2\sigma^2}\times e^{x^T y/2\sigma^2} \times e^{-||y||^2/2\sigma^2}$ is valid. (1) $x^T y$ linear kernel is valid (2) then $\exp(\frac{1}{\sigma^2}x^T y)$ is valid, (3) let $f(x) = \exp(-\frac{1}{2\sigma^2} ||x||^2)$, by rules $f(x) k(x, y) f(y)$ RBF is valid.

\textbf{Mercer's Theorem}: Assume $k(x, x^\prime)$ is a valid kernel. Then there exists an orthogonal basis $e_i$ and $\lambda_i\ge 0$, s.t. $k(x, x^\prime) = \sum_{i} \lambda_i e_i(x) e_i(x^\prime)$.


