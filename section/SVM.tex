\section{Support Vector Machine}

\subsection*{Hard-Margin SVM}

The optimization form is to minimize $\frac{1}{2}||w||^2$, s.t. $y_i(w^T x_i+w_0)\ge 1$ ($y_i\in \{-1,1\}$). It is convex and the Slater's condition holds under the assumption of linear separability.

By solving $\min_{w, w_0} L(w, w_0, \alpha)$, the duality form is $\max_\alpha -\frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j x_i^T x_j+\sum_i \alpha_i$, s.t. $\alpha_i\ge 0$, $\sum_i \alpha_i y_i = 0$. $w^* = \sum_{i\in \text{support vec}} \alpha_i^* y_i x_i$.

\subsection*{Soft-Margin SVM \& Kernel SVM}

When the data is not linearly separable, one solution is to add slack variables: $\min_{\xi,w,w_0} \frac{1}{2}||w||^2+C\sum_i \xi_i$, s.t. $y_i(w^T x_i+w_0) \ge 1-\xi_i$ and $\xi_i\ge 0$. Note: when the data is linearly separable, soft-margin does not necessarily produce the same cut-plane as the hard-margin SVM, especially when the margin is small due to few support vectors. When $C\rightarrow\infty$, it becomes hard-margin SVM.

The only difference in the dual form is that we need an extra condition $0\le \alpha\le C$. $\xi_i^* = \max(0, 1-y_i(w^{*T}x_i+w_0))$.

Another solution, Kernel SVM, is to use $K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$ to replace $x_i^T x_j$.

\subsection*{Multi-class SVM}

Idea: use M := \#class hyperplanes and maximize the generalized margin: $\min \sum_{i=1}^M w_i^T w_i$ s.t. $w_{y_i}^T x_i + w_{y_i, 0} - \max_{y\ne y_i} w_y^T x_i + w_{y,0} \ge 1$.

\subsection*{Structural SVM}

Goal: predict a structured output label, e.g., parsing trees.

\inLongVersion{
Difficulty of predicting structures:
\begin{enumerate}
    \item Compact representation of output space (cannot use one model for each class). Sol: use a score function and feature map $\hat{y}=\argmax_y w^T \Psi(x, y)$.
    \item Efficient prediction (cannot use brute-force search to find argmax). Sol: assume structures such as decomposable output spaces.
    \item Define prediction error (cannot use 0/1 errors). Sol: use a loss function $\Delta(y, \hat{y})$.
    \item Efficient training (cannot evaluate all constraints). Sol: iteratively add new constraints to the training.
\end{enumerate}
}